{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install optuna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install GML","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install sweetviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# import sweetviz as sv\n\n# from GML.Ghalat_Machine_Learning import Ghalat_Machine_Learning\n\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.cluster import KMeans\n\nimport tqdm\n\n# import optuna\n\nimport multiprocessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_metric(y_true,y_pred):\n    return 100*np.sqrt(mean_squared_log_error(y_true,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/train_0irEZ2H.csv')\ntest = pd.read_csv('/kaggle/input/test_nfaJ3J5.csv')\nsample = pd.read_csv('/kaggle/input/sample_submission_pzljTaX.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['week'] = pd.to_datetime(train['week'])\ntest['week'] = pd.to_datetime(test['week'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['day'] = train['week'].dt.day\ntrain['month'] = train['week'].dt.month\ntrain['year'] = train['week'].dt.year\n# train['dow'] = train['week'].dt.dayofweek\n\ntest['day'] = test['week'].dt.day\ntest['month'] = test['week'].dt.month\ntest['year'] = test['week'].dt.year\n# test['dow'] = test['week'].dt.dayofweek\n\n\ntrain.drop(['week'],axis=1,inplace=True)\ntest.drop(['week'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# r = sv.compare([train,'train'],[test,'test'],'units_sold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# r.show_html('report_train_test.html')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['b_m_t'] = (train['base_price'] - train['total_price'])/train['base_price']\ntest['b_m_t'] = (test['base_price'] - test['total_price'])/test['base_price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def price_bin(s):\n    if 100 <= s <= 250:\n        return 1\n    else:\n        return 0\n\n# train['p_bin'] = train['total_price'].apply(price_bin)\n# test['p_bin'] = test['total_price'].apply(price_bin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nmp = train.groupby('store_id')['units_sold'].mean()\ntrain['store_avg'] = train['store_id'].map(mp)\ntest['store_avg'] = test['store_id'].map(mp)\n\nmp = train.groupby('store_id')['units_sold'].min()\ntrain['store_min'] = train['store_id'].map(mp)\ntest['store_min'] = test['store_id'].map(mp)\n\nmp = train.groupby('store_id')['units_sold'].max()\ntrain['store_max'] = train['store_id'].map(mp)\ntest['store_max'] = test['store_id'].map(mp)\n\"\"\"\npass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sku = np.unique(train['sku_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfor sid in stores:\n    gml = Ghalat_Machine_Learning()\n    t = train[train['store_id']==sid].copy()\n    X = t.drop('units_sold',axis=1)\n    y = t['units_sold'].copy()\n    gml.GMLRegressor(X, y, metric = metric, neural_net = 'yes', verbose = False)\n\"\"\"\npass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['units_sold'] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stores = np.unique(train['store_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def st_cat(s):\n    return int(s/100)\ndef r_st_cat(s):\n    return int(s%10)\n\ntrain['st_cat'] = train['store_id'].apply(st_cat)\ntest['st_cat'] = test['store_id'].apply(st_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfor sk in tqdm.tqdm(sku):\n    for store in stores:\n        tr = train[(train['sku_id']==sk) & (train['store_id']==store)].copy()\n        ts = test[(test['sku_id']==sk) & (test['store_id']==store)].copy()\n        \n        if tr.empty or ts.empty:\n            continue\n            \n        mp = tr.groupby('store_id')['units_sold'].mean()\n        tr['store_avg'] = tr['store_id'].map(mp)\n        ts['store_avg'] = ts['store_id'].map(mp)\n\n        mp = tr.groupby('store_id')['units_sold'].std()\n        tr['store_std'] = tr['store_id'].map(mp)\n        ts['store_std'] = ts['store_id'].map(mp)\n\n        mp = tr.groupby('store_id')['units_sold'].min()\n        tr['store_min'] = tr['store_id'].map(mp)\n        ts['store_min'] = ts['store_id'].map(mp)\n\n        mp = tr.groupby('store_id')['units_sold'].max()\n        tr['store_max'] = tr['store_id'].map(mp)\n        ts['store_max'] = ts['store_id'].map(mp)\n\n        s_y = tr.groupby('year')['units_sold'].mean()\n        tr['sales_yearly'] = tr['year'].map(s_y)\n        ts['sales_yearly'] = ts['year'].map(s_y)\n\n        X = tr.drop(['units_sold'],axis=1)\n        X.fillna(0,inplace=True)\n        y = tr['units_sold'].copy()\n        tes = ts.drop(['units_sold'],axis=1)\n\n        model = ExtraTreesRegressor()\n        model.fit(X, y)\n        preds = model.predict(tes)\n\n        test.at[ts.index.values,'units_sold'] = preds\n\"\"\"\npass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import BayesianRidge, LinearRegression\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rounding(num): # magic function. used it during blending\n    round_num, round_num2 = 0, 0\n    uniques = np.unique(train['units_sold'])\n    for i,n in enumerate(uniques):\n        if n > num:\n            break\n        round_num = n\n        round_num2 = uniques[i+1]\n    return (round_num+round_num2)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def partition(price,val):\n    if price <= val:\n        return 1\n    else:\n        return 0\n\ndef partition_val(price):\n    return np.mean(price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def change_shop(s):\n    if np.abs(s['total_price']-s['base_price']) > 0 and s['is_featured_sku'] == 0 :\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for store in tqdm.tqdm(sku):\n    tr = train[train['sku_id']==store].copy()\n    ts = test[test['sku_id']==store].copy()\n    \n    tr.sort_values('store_id',inplace=True)\n    ts.sort_values('store_id',inplace=True)\n    \n    mp = tr.groupby('store_id')['units_sold'].mean()\n    tr['store_avg'] = tr['store_id'].map(mp)\n    ts['store_avg'] = ts['store_id'].map(mp)\n    \n    mp = tr.groupby('store_id')['units_sold'].std()\n    tr['store_std'] = tr['store_id'].map(mp)\n    ts['store_std'] = ts['store_id'].map(mp)\n\n    mp = tr.groupby('store_id')['units_sold'].min()\n    tr['store_min'] = tr['store_id'].map(mp)\n    ts['store_min'] = ts['store_id'].map(mp)\n\n    mp = tr.groupby('store_id')['units_sold'].max()\n    tr['store_max'] = tr['store_id'].map(mp)\n    ts['store_max'] = ts['store_id'].map(mp)\n    \n    s_y = tr.groupby('year')['units_sold'].mean()\n    tr['sales_yearly'] = tr['year'].map(s_y)\n    ts['sales_yearly'] = ts['year'].map(s_y)\n    \n    tr['pct_change'] = tr['total_price'].pct_change()\n    ts['pct_change'] = ts['total_price'].pct_change()\n    \n    tr['is_B'] = (tr['is_featured_sku'] & tr['is_display_sku'])\n    ts['is_B'] = (ts['is_featured_sku'] & ts['is_display_sku'])\n    \n    tr['avg_change'] = (tr['total_price'] + tr['base_price'])/2\n    ts['avg_change'] = (ts['total_price'] + ts['base_price'])/2\n    \n    tr['ExtraTax'] = tr.apply(change_shop,axis=1)\n    ts['ExtraTax'] = ts.apply(change_shop,axis=1)\n    \n    lr = BayesianRidge()\n    \n    X = tr.drop(['units_sold','sku_id'],axis=1)\n    X.fillna(0,inplace=True)\n    y = tr['units_sold'].copy()\n    lr.fit(X, y)\n    X['pred'] = lr.predict(X)\n    # X['pred'] = X['pred'].apply(rounding)\n    tes = ts.drop(['units_sold','sku_id'],axis=1)\n    tes.fillna(0,inplace=True)\n    tes['pred'] = lr.predict(tes)\n    # tes['pred'] = tes['pred'].apply(rounding)\n    \n    model1 = RandomForestRegressor(n_estimators=500,min_samples_leaf=5,n_jobs=-1)\n    model2 = ExtraTreesRegressor(n_jobs=-1,min_samples_leaf=3)\n    model1.fit(X, y)\n    model2.fit(X, y)\n    preds = (model1.predict(tes) + model2.predict(tes))/2 \n    \n    test.at[ts.index.values,'units_sold'] = preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test['units_sold'] -= 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['record_ID','units_sold']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['units_sold'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['units_sold'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = pd.DataFrame()\nimp['f'] = X.columns\nimp['i'] = model1.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp.sort_values('i',ascending = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}